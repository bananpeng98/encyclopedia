#+title: Stochastic sis model
#+roam_tags: sis model stochastic population dynamic system

* Setup :noexport:
#+call: init()
#+call: init-plot-style()

* Lib :noexport:
:PROPERTIES:
:header-args: :tangle encyclopedia/stochastic_sis_model.py :results silent
:END:

#+begin_src jupyter-python
import matplotlib.pyplot as plt
import numpy as np
from sympy import *
from sympy.stats import *
from pyorg.latex import *
from encyclopedia.deterministic_sis_model import *
from scipy.optimize import curve_fit
#+end_src

* Stochastic sis model
The definition of the stochastic version of the sis model is a Markov chain and
can be described like this
#+begin_src jupyter-python
N, TI, TR, m = symbols('N T_I T_R m', integer=True)
alpha, beta, t = symbols('alpha beta t', real=True, positive=True)
n = Idx('n', m)
dt = Symbol('dt')
b, d, c, Pb, Pd = symbols('b d c P_b P_d', shape=(1,), cls=IndexedBase)
trns = lambda tr, op, rate: LBiOp(tr, op, rate, separator=':')
deterministic_model = DeterministicSISModel()

rates_lhs = Matrix([b[n], d[n]])
rates_rhs = Matrix([
    alpha*n*(1-n/N),
    beta*n,
])
nexts = Matrix([
    Lambda(n, n+1),
    Lambda(n, n-1),
])

LMatColon(nexts, LMatEq(rates_lhs, rates_rhs))
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{array}{l}
\left( n \mapsto n + 1 \right) : {b_{n}} = \alpha \left(1 - \frac{n}{N}\right) n\\
\left( n \mapsto n - 1 \right) : {d_{n}} = \beta n
\end{array}\end{equation}
:END:
where $n$ is $I_t$ for simplicity. The expression to the left of the colon
defines an event and the right side is the rate of the event occurring.

** Deriving deterministic model and master equation
The /master equation/ of the system looks like the following
#+begin_src jupyter-python
rho = symbols('rho', cls=Function)
Ep, Em, Epm = symbols('E^+ E^- E^{\\pm}')
master_dt = LEq(rho(n, t+dt), rho(n, t)+dt*((b[n-1]*rho(n-1,t)+d[n+1]*rho(n+1,t))-(b[n]*rho(n, t)+d[n]*rho(n, t))))
master_eq = LComposeStep(
    LStep(lambda e: e/dt),
    LSimplifyStep(),
    LSubsStep(rho(n, t+dt)/dt, rho(n, t).diff(t)),
    LSubsStep(rho(n, t)/dt, 0)
)(master_dt)
master_eq.factor(rho(n,t))
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \rho{\left(n,t \right)}      =      \left(- {b_{n}} - {d_{n}}\right) \rho{\left(n,t \right)} + \rho{\left(n - 1,t \right)} {b_{n - 1}} + \rho{\left(n + 1,t \right)} {d_{n + 1}}\end{equation}
:END:

We can rewrite this equation using /step operators/ like below
#+begin_src jupyter-python
master_step = LEq(master_eq.lhs, (1-Ep)*rho(n,t)*b[n]+(1-Em)*rho(n,t)*d[n])
master_step
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \rho{\left(n,t \right)} = \left(1 - E^{+}\right) \rho{\left(n,t \right)} {b_{n}} + \left(1 - E^{-}\right) \rho{\left(n,t \right)} {d_{n}}\end{equation}
:END:

To simplify the expansion of the equation, we introduce $\hat{I}=n/N$ and get $b(\hat{I})$
and $d(\hat{I})$, substituting this we get
#+begin_src jupyter-python
Ih = Symbol('\\hat{I}')
bf, df = symbols('b d', cls=Function)
master_subs = LComposeStep(
    # LSubsStep(list(zip(rates_lhs, rates_rhs))),
    # LSubsStep(list(zip(rates_lhs.subs(n, n-1), rates_rhs.subs(n, n-1)))),
    # LSubsStep(list(zip(rates_lhs.subs(n, n+1), rates_rhs.subs(n, n+1)))),
    LSubsStep([(b[n], bf(Ih)), (d[n], df(Ih))]),
    LSubsStep(n/N, Ih),
    # LSimplifyStep()
)(master_step)
master_subs
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \rho{\left(n,t \right)}   =   \left(1 - E^{+}\right) b{\left(\hat{I} \right)} \rho{\left(n,t \right)} + \left(1 - E^{-}\right) d{\left(\hat{I} \right)} \rho{\left(n,t \right)}\end{equation}
:END:

We can represent the action of $E^+$ and $E^-$ on a smooth function $g(I)$ like
this
#+begin_src jupyter-python
g = Function('g')
k = Idx('k')
dIop = Symbol('\\frac{\\partial}{\\partial \\hat{I}}')
Epm_eq = lambda E, pm: LEq(E*g(Ih),
                           g(Ih+pm*1/N),
                           Sum(((pm/N)/factorial(k))*g(Ih).diff((Ih, k)), (k, 0, oo)),
                           exp(pm*(1/N)*dIop)*g(Ih))
LArray(Epm_eq(Ep, 1), Epm_eq(Em, -1))
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{array}{l}
E^{+} g{\left(\hat{I} \right)} = g{\left(\hat{I} + \frac{1}{N} \right)} = \sum_{k=0}^{\infty} \frac{\frac{d^{k}}{d \hat{I}^{k}} g{\left(\hat{I} \right)}}{N k!} = g{\left(\hat{I} \right)} e^{\frac{\frac{\partial}{\partial \hat{I}}}{N}}\\
E^{-} g{\left(\hat{I} \right)} = g{\left(\hat{I} - \frac{1}{N} \right)} = \sum_{k=0}^{\infty} - \frac{\frac{d^{k}}{d \hat{I}^{k}} g{\left(\hat{I} \right)}}{N k!} = g{\left(\hat{I} \right)} e^{- \frac{\frac{\partial}{\partial \hat{I}}}{N}}
\end{array}\end{equation}
:END:

Expanding using these definitions we get
#+begin_src jupyter-python
master_action = master_subs.subs([(Ep, Epm_eq(Ep, 1).rhs), (Em, Epm_eq(Em, -1).rhs)]).subs(g(Ih), 1)
master_action
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \rho{\left(n,t \right)}     =     \left(1 - e^{- \frac{\frac{\partial}{\partial \hat{I}}}{N}}\right) d{\left(\hat{I} \right)} \rho{\left(n,t \right)} + \left(1 - e^{\frac{\frac{\partial}{\partial \hat{I}}}{N}}\right) b{\left(\hat{I} \right)} \rho{\left(n,t \right)}\end{equation}
:END:

This equation corresponds to a /transport equation/ an is approximately equal to
#+begin_src jupyter-python
master_trans = LApprox(master_action.lhs, Derivative(df(Ih)-bf(Ih), Ih)*rho(n,t))
master_trans
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \rho{\left(n,t \right)} \approx \rho{\left(n,t \right)} \frac{d}{d \hat{I}} \left(- b{\left(\hat{I} \right)} + d{\left(\hat{I} \right)}\right)\end{equation}
:END:

The transport equation formulated in deterministic dynamics looks like this
#+begin_src jupyter-python
master_dynamic = LEq(Derivative(Ih, t), df(Ih)-bf(Ih), rates_rhs[0].subs(n/N, Ih)-rates_rhs[1].subs(n/N, Ih))
master_dynamic
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\frac{d}{d t} \hat{I} = - b{\left(\hat{I} \right)} + d{\left(\hat{I} \right)} = \alpha \left(1 - \hat{I}\right) n - \beta n\end{equation}
:END:

Substituting $\hat{I}=n/N=I(t)/N=\frac{I(t)}{I(t)+S(t)}$ back into the equation we get
#+begin_src jupyter-python
LSteps(master_dynamic.lhs, col_join=LEq)(
    LSubsStep(master_dynamic.lhs, master_dynamic.rhs),
    LSubsStep(Ih, n/N),
    LSubsStep(n, I(t)),
    LSubsStep(N, I(t)+S(t)),
    LStep(lambda e: e.apart(alpha)),
    I(t).diff(t)
)
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{array}{l}
\frac{d}{d t} \hat{I} = \\
\quad = \alpha \left(1 - \hat{I}\right) n - \beta n  =  \\
\quad = \alpha \left(1 - \frac{n}{N}\right) n - \beta n   =   \\
\quad = \alpha \left(1 - \frac{I{\left(t \right)}}{N}\right) I{\left(t \right)} - \beta I{\left(t \right)}    =    \\
\quad = \alpha \left(1 - \frac{I{\left(t \right)}}{I{\left(t \right)} + S{\left(t \right)}}\right) I{\left(t \right)} - \beta I{\left(t \right)}     =     \\
\quad = \frac{\alpha I{\left(t \right)} S{\left(t \right)}}{I{\left(t \right)} + S{\left(t \right)}} - \beta I{\left(t \right)}      =      \\
\quad = \frac{d}{d t} I{\left(t \right)}
\end{array}\end{equation}
:END:
which is the same as the deterministic model.

However, this does not mean that the stochastic model will behave exactly like
the deterministic model, the stochastic model actually has a /quasi-steady
states/. This means that some steady states seems to be stable for a while but
can suddenly diverge. We will investigate this in the next section when we
simulate the stochastic model.

** Simulation
To simulate the Markov chain, we need to know the probability going from one
state of $n$ to another. A simple method is to just multiply each rate by a
small time step like below

#+begin_src jupyter-python
nexts_full = nexts.row_insert(2, Matrix([Lambda(n, Add(n, 0, evaluate=False))]))
rates_rhs_full = rates_rhs.row_insert(2, Matrix([b[n]+d[n]]))
rates_lhs_full = rates_lhs.row_insert(2, Matrix([b[n]+d[n]]))
prob_lhs = nexts_full.applyfunc(Probability)
prob_rhs = rates_lhs_full*dt
prob_rhs[2] = 1-prob_rhs[2]
LMatEq(prob_lhs, prob_rhs)
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{array}{l}
P[\left( n \mapsto n + 1 \right)] = dt {b_{n}}\\
P[\left( n \mapsto n - 1 \right)] = dt {d_{n}}\\
P[\left( n \mapsto n + 0 \right)] = - dt \left({b_{n}} + {d_{n}}\right) + 1
\end{array}\end{equation}
:END:

Using the probabilities, we can simulate an event happening by comparing the
probability with a uniform random variable $r$ in the range $(0, 1)$. See Figure
[[src:fig:stoch_simulation]] for a simulation of the system compared to the
deterministic model. If we run the model for longer (see Figure
[[fig:stoch_quasi]]), we can see that it is only a quasi-steady state since the
infected population eventually dies out.

#+begin_src jupyter-python :exports none
prob_bd = lambdify((b[n], d[n], n, N, dt), prob_rhs)
prob_bd(0.1, 0.2, 2, 10, 0.1).shape
#+end_src

#+RESULTS:
| 3 | 1 |

#+begin_src jupyter-python :exports none
prob = lambdify((alpha, beta, n, N, dt), prob_rhs.subs(zip(rates_lhs, rates_rhs)))
prob(0.5, 0.1, 2, 10, 0.1)
#+end_src

#+RESULTS:
: array([[0.08],
:        [0.02],
:        [0.9 ]])

#+begin_src jupyter-python
def step(alpha, beta, n, N, dt, prob_n):
    should_transition = np.random.random(size=prob_n.shape) < prob_n
    for i, nn in zip(range(3), [1, -1, 0]):
        n += should_transition[i]*nn
    return n
#+end_src

#+RESULTS:

#+name: src:fig:stoch_simulation
#+begin_src jupyter-python :noweb yes :results output
n_n = 1
alpha_n = 0.5
V = {
    N: 100,
    I0: 10,
    alpha: 0.5,
    beta: 0.1,
    dt: 0.05
}
V[n] = V[I0]
V[S0] = V[N] - V[I0]
n_n = np.array([V[n], V[n]])
n_hist = [n_n.copy()]
times = [0]
t_n = 0
steps = 1000
for _ in range(steps):
    prob_n = np.squeeze(prob(V[alpha], V[beta], n_n, V[N], V[dt]))
    t_n += V[dt]
    step(V[alpha], V[beta], n_n, V[N], V[dt], prob_n)
    n_hist.append(n_n.copy())
    times.append(t_n)

times = np.array(times)
n_hist = np.array(n_hist)
plt.plot(times, V[N]-n_hist[:, 0], color=<<color("green")>>, label="S stochastic", lw=0.5)
plt.plot(times, n_hist[:, 0], color=<<color("red")>>, label="I stochastic", lw=0.5)
deterministic_model.plot([0, steps*V[dt]], V, ls='--', alpha=0.8, lw=1.0)
plt.xlabel("$t$")
plt.ylabel("population")
plt.legend()
#+end_src

#+caption: A simulation of the stochastic system compared to the deterministic one.
#+attr_latex: scale=0.75
#+label: fig:stoch_simulation
#+RESULTS: src:fig:stoch_simulation
[[file:./.ob-jupyter/adfebca63a2f31d083a0a655f2f8e50452f95e91.png]]

#+name: src:fig:stoch_quasi
#+begin_src jupyter-python :noweb yes :results output
V = {
    N: 100,
    S0: 95,
    I0: 5,
    n: 5,
    alpha: 0.8,
    beta: 0.6,
    dt: 0.05
}
V[I0] = int(V[N]*(V[alpha]-V[beta])/V[alpha])
V[n] = V[I0]
V[S0] = V[N] - V[I0]
n_n = np.array([V[n], V[n]])
n_hist = [n_n.copy()]
times = [0]
t_n = 0
steps = 10000
for _ in range(steps):
    prob_n = np.squeeze(prob(V[alpha], V[beta], n_n, V[N], V[dt]))
    t_n += V[dt]
    step(V[alpha], V[beta], n_n, V[N], V[dt], prob_n)
    n_hist.append(n_n.copy())
    times.append(t_n)

times = np.array(times)
n_hist = np.array(n_hist)
plt.plot(times, V[N]-n_hist[:, 0], color=<<color("green")>>, label="S stochastic", lw=0.5)
plt.plot(times, n_hist[:, 0], color=<<color("red")>>, label="I stochastic", lw=0.5)
deterministic_model.plot([0, steps*V[dt]], V, ls='--', alpha=0.8, lw=1.0)
plt.xlabel("$t$")
plt.ylabel("population")
plt.legend()
#+end_src

#+caption: A longer simulation of the stochastic system showing the increasing fluctuations that results in the infected population to die out.
#+attr_latex: scale=0.75
#+label: fig:stoch_quasi
#+RESULTS: src:fig:stoch_quasi
[[file:./.ob-jupyter/0587960cff3332cde172db83712e55198e3febf4.png]]

** Efficient simulation
We can make our simulation more efficient by realizing the similarity of the
algorithm to the one generating numbers from the exponential distribution. The
hypothesis is that we can simply sample from an exponential distribution to get
the time of the next event instead of simulating the whole system.

We will try and find the parameters for the distribution in this section.

#+begin_src jupyter-python
bd_cases = [
    {b[n]: 0.1, d[n]: 0.2},
    {b[n]: 1.0, d[n]: 2.0},
    {b[n]: 10.0, d[n]: 5.0},
]
#+end_src

#+RESULTS:

#+begin_src jupyter-python :exports none
eq = LMatEq(rates_rhs, [Number(0.1), Number(0.2)])
eq
sol = solve([Eq(alpha*(1-n/N), b[n]), Eq(beta*n, d[n])], [alpha, beta])
alpha_bn = sol[alpha]
beta_bn = sol[beta]
LValues(sol)
#+end_src

#+RESULTS:
:RESULTS:
\begin{equation}\begin{cases}
\alpha = - \frac{N {b_{n}}}{- N + n}\\
\beta = \frac{{d_{n}}}{n}
\end{cases}\end{equation}
:END:

#+begin_src jupyter-python :noweb yes :results silent
def plot_dist_sims(axes, results, cases, bins='auto'):
    for i, (ts, case, axcol) in enumerate(zip(results, bd_cases, axes.T)):
        b_n = case[b[n]]
        d_n = case[d[n]]
        for ax, ti, lab, rate_txt, rate in zip(axcol, range(2), ['t_b', 't_d'], ['b_n', 'd_n'], case.values()):
            ax.set_title(latex(LValues(case, join=LComma), mode='inline'))
            hist, counts = np.histogram(ts[:, :, ti], bins=bins, density=True)
            t_lin = np.linspace(ts[:, :, ti].min(), ts[:, :, ti].max(), len(hist))
            (A, B), _ = curve_fit(lambda t,A,B: A*np.exp(-B*t), t_lin, hist, p0=(rate, rate))
            ax.plot(t_lin, A*np.exp(-B*t_lin), label=f"best fit$={A:.4f}e^{{{-B:.4f}t}}$", color=<<color("blue")>>)
            ax.plot(t_lin, rate*np.exp(-rate*t_lin), label=f"theoretical$={rate_txt}e^{{-{rate_txt}t}}$", color=<<color("blue")>>, ls='--')
            ax.scatter(t_lin, hist)
            ax.set_yscale('log')
            ax.set_xlabel(f"${lab}$")
            ax.set_ylabel(f"$log(P({lab}))$")
            ax.legend()
#+end_src


# Calculate by continuing time and taking differences
#+begin_src jupyter-python
def calc_times(prob_n, points, steps):
    times = np.zeros([steps, points, 2])
    t = np.zeros([points, 2])
    t_prev = np.zeros([points, 2])
    index = np.zeros([points, 2], dtype=int)
    n_n = np.zeros([points])
    k = 0
    while (index < steps).any():
        t += dt_n
        should_transition = np.random.random(size=[points, 2]) < prob_n
        for i in range(2):
            curr_trans = should_transition[:, i]&(index[:, i]<steps)
            if curr_trans.sum() > 0:
                curr_index = index[curr_trans, i]
                times[curr_index, curr_trans, i] = t[curr_trans, i]-t_prev[curr_trans, i]
                t_prev[curr_trans, i] = t[curr_trans, i]
                index[curr_trans, i] += 1
        k += 1
    return times
#+end_src

#+RESULTS:

#+begin_src jupyter-python :results output :noweb yes :eval never-export
results = []
points = 100
steps = 100
dt_n = 0.001
for case in bd_cases:
    b_n = case[b[n]]
    d_n = case[d[n]]
    results.append(calc_times(np.array([b_n*dt_n, d_n*dt_n]), points, steps))

fig, axs = plt.subplots(2, 3, figsize=(4*3, 4*2))
fig.suptitle(f"$dt={dt_n}$, samples=${points*steps}$")
plot_dist_sims(axs, results, bd_cases, bins=100)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/7631f7314416378dfd0853697048676f0f0b8ce4.png]]
